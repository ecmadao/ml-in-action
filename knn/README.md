## K Nearest Neighbor

K 临近算法，分类学习中的经典算法，属于监督学习。注意其和 K 均值算法（K-Means）不同，K-Means 用于解决聚类问题，为无监督学习。

- KNN：监督学习。已知数据集中各元素分类，现给出一个新的元素，为它进行分类
- K-Means：无监督学习。给出一个数据集，对数据集中的各元素进行聚合，生成 K 个种类

对于分类、聚类问题，很常见的做法是使用点点直接的距离（闵可夫斯基距离）来表示元素间的相似度。但这种算法有一个普遍的问题：当不同特征的取值范围有很大差别的时候，较大的那个数据造成对算法造成的影响远远大于较小的那个数据。

举个栗子：关于房价预测的算法（入门机器学习时很常见的案例了吧），有 “房间数” 和 “房屋面积” 两个特征。房间数的取值区间为 `[1, 5]`，而房屋面积的区间则为 `[100, 1000]`，此时，如果照以前那样代入计算，则 “房间数” 这个特征对你算法的影响几乎可以不算。虽然这不是一个分类算法，但对于分类算法而言，这种问题也是存在的。

这种时候，可以采取[特征缩放](https://zh.wikipedia.org/zh/特征缩放)的手段来预处理数据集；或者针对分类/聚类算法，使用向量余弦法：

对于下面两个数据，它们都包含了一些特征，并由此进行分类（是什么类型的房屋）比如：

```javascript
// [房间数, 小区人口数, 房屋面积, 有独卫] ==> 民宅
const a = [3, 3000, 100, 1];
// [房间数, 小区人口数, 房屋面积, 没有独卫] ==> 青年公寓
const b = [1, 2600, 20, 0];
```

将它们放在空间坐标系后，成为两个向量。而这两个向量之间的夹角则可以很好的体验出两者之间的相似度。这样的方法就避免了较大数据造成的影响。

### Articles

- [K NEAREST NEIGHBOR 算法](http://coolshell.cn/articles/8052.html)

### Code

```bash
# run in local
$ git clone git@github.com:ecmadao/ml-in-action.git
$ cd ml-in-action
$ npm i
$ node knn/code/index.js
```
